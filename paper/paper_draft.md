# Daily-Outcome Offline Reinforcement Learning for Trading-Advice LLMs
**A Lightweight Continuous Learning Loop on Consumer Hardware**

Rohit Krishnan¹  
¹ Independent Researcher

### Abstract
We introduce a fully automated, daily closed loop for live financial headlines: ingest, structured roll‑out generation, next‑day LLM‑based scoring, and overnight GSPO‑inspired updates on real, noisy market outcomes. Each 24 h cycle processes ≈30 streaming financial headlines into eight Qwen‑3‑0.6B roll‑outs per headline, grades them via a robust evaluator, and folds delayed outcome feedback into an offline, group‑based sequence objective (GSPO‑inspired; \cite{Zheng2025GSPO}). A strict evaluation fix (enforcing 1–8 ranks with deduplication, clamping, and deterministic padding) eliminated label noise and stabilized training. Over August 4–11, 2025 (post‑fix), rewards in training data remained bounded in [0,1] with no negatives, and in‑epoch average reward rose steadily while a training‑time loss proxy (reward‑weighted NLL over response tokens) increased moderately—indicating healthy specialization without collapse. We release code, sample data, and figure scripts as a reproducible blueprint for domain‑driven offline RL.

<!-- Figures are stored in paper/figs/ and generated by scripts/make_figs.py -->

## 1  Introduction
Financial news streams provide a natural supervision signal for next‑day, market‑relevant predictions. We operationalize this by (i) collecting daily headlines, (ii) generating eight structured trading predictions per headline using a compact LLM (Qwen‑3‑0.6B/MLX), (iii) ranking predictions with the next day’s headlines, and (iv) optimizing the policy with a GSPO‑inspired offline update. The 24‑hour delay between prediction and outcome enables a safe, offline learning regime without on‑policy leakage of future information.

Our key contributions are:
- **Continuous Daily Learning**: A fully automated pipeline that processes ≈30 headlines/day, generates 8 diverse predictions/headline, and updates the model overnight from next‑day outcomes.
- **Offline RL with Delayed Rewards**: A GSPO‑inspired objective tailored to delayed, offline updates on precomputed roll‑outs.
- **Robust Automatic Evaluation**: A strict, rank‑based evaluator (1–8 with dedupe/clamp/pad) that tolerates real‑world format drift and eliminates invalid labels.
- **Consumer Hardware Deployment**: End‑to‑end continual tuning on a single Apple M4 Max, democratizing RL‑for‑LLMs without clusters.

## 2  System Overview
- Model: Qwen/Qwen3‑0.6B via MLX/MLX‑LM.
- Training: GSPO‑inspired, response‑only loss, 1 epoch/day.
- Generation: 8 stochastic roll‑outs/headline (temp=0.8, top_p=0.95, top_k=50).
- Evaluation: Iterative A–H selection; strict final 1–8 ranks (dedupe, clamp [1,8], pad unpicked=7).
- Checkpointing: dated `final_model_YYYYMMDD/` + rolling `final_model/`.
- Namespaces: base `timestamped_storage/`; isolated runs under `timestamped_storage_NEWRUN/`.

## 3  Data & Timeline (Aug 2 → Aug 11, 2025)
Post‑fix training JSON statistics (per day):
- 2025‑08‑04: n=256, avg=0.50, min=0.0, max=1.0, negatives=0
- 2025‑08‑05: n=224, avg=0.50, min=0.0, max=1.0, negatives=0
- 2025‑08‑06: n=216, avg=0.50, min=0.0, max=1.0, negatives=0
- 2025‑08‑07: n=280, avg=0.50, min=0.0, max=1.0, negatives=0
- 2025‑08‑08: n=248, avg=0.50, min=0.0, max=1.0, negatives=0
- 2025‑08‑10: n=144, avg=0.50, min=0.0, max=1.0, negatives=0 (evaluated with 2025‑08‑11 headlines via override)

Today’s generation (2025‑08‑11): 248 roll‑outs; avg structure score ≈ 0.826.

## 4  Results
### 4.1  Training dynamics (epoch summaries)
- 2025‑08‑04: steps≈1480, avg_reward≈0.0135, avg_loss≈4.52, improvement≈+0.0021
- 2025‑08‑05: steps≈1704, avg_reward≈0.0183, avg_loss≈5.99, improvement≈+0.0145
- 2025‑08‑06: steps≈1920, avg_reward≈0.0219, avg_loss≈7.05, improvement≈+0.0238
- 2025‑08‑07: steps≈2200, avg_reward≈0.0254, avg_loss≈8.16, improvement≈+0.0358
- 2025‑08‑08: steps≈2448, avg_reward≈0.0279, avg_loss≈8.88, improvement≈+0.0441
- 2025‑08‑10: steps≈2592, avg_reward≈0.0292, avg_loss≈9.01, improvement≈+0.0417

Observation: In‑epoch average reward increases monotonically; the loss proxy grows moderately—consistent with stable specialization without collapse.

### 4.2  Evaluation integrity
Strict rank enforcement eliminated invalid labels and negative rewards. Rankings are not trivially sequential; varied permutations appear in logs. Training JSONs show clean [0,1] rewards with negatives=0 across days.

## 5  Engineering & Reproducibility
- Environment: MLX 0.26.1, MLX‑LM 0.25.2, Transformers 4.51.3; Model `Qwen/Qwen3‑0.6B`; Seed 1234.
- Daily update (yesterday→today):
```
python run_next_update.py --resume_from_last_model --run_suffix NEWRUN --seed 1234
```
- Backfill (isolated NEWRUN):
```
python run_backfill_newrun.py --start_date 20250804 --end_date 20250808 \
  --resume_from_last_model --force_evaluations --force_training \
  --run_suffix NEWRUN --seed 1234
```
- Override evaluations (evaluate D with headlines from H):
```
python run_override_update.py --prediction_date D --headlines_date H \
  --resume_from_last_model --run_suffix NEWRUN --seed 1234
```
- Versioning: rolling `final_model/` plus dated `final_model_YYYYMMDD/`.

## 6  Limitations & Future Work
- The evaluator occasionally requires retries; logging retry counts and agreement would refine quality metrics.
- Log both scaled (step‑time) and unscaled rewards for clarity during GSPO.
- Add A/B side‑by‑side behavioral evals (baseline vs latest) and economic back‑tests.
- Explore EMA baselines and prompt‑masked vs full‑loss variants systematically.

## 7  Conclusion
With strict evaluation and clean reward mapping, continuous GSPO‑style training on Qwen‑3‑0.6B is stable and improving. The pipeline runs reproducibly day‑to‑day, updates the model, and preserves dated checkpoints for research reporting on consumer hardware.

## References
- Zheng et al., "Group Sequence Policy Optimization (GSPO)", 2025.

---

## 3  End-to-End Pipeline & Methodology

### 3.1  Daily data flow

```mermaid
graph TD
  A[RSS Feeds<br>(~30–40 headlines)] --> B[Roll-out Generator<br>8 × predictions]
  B --> C[Outcome Tracker<br>+24 h headlines]
  C --> D[LLM Evaluator<br>Rank A–H → dense reward]
  D --> E[GSPO Trainer<br>offline RL step]
  E --> B
```

The loop executes once per calendar day, forming a **24 h delayed reward** scenario—naturally suited to offline RL, since on‑policy interaction would leak future information.

### 3.2  Complete Pipeline Pseudocode

```python
def daily_pipeline(date: str):
    # 1. Collect today's headlines
    headlines = collect_rss_feeds(date)
    
    # 2. Generate 8 diverse predictions per headline
    predictions = []
    for headline in headlines:
        rollouts = generate_rollouts(
            model=current_model,
            prompt=headline,
            num_rollouts=8,
            temperature=0.7
        )
        predictions.extend(rollouts)
    
    # 3. Store for next-day evaluation
    store_predictions(date, predictions)
    
    # 4. If we have next-day headlines, evaluate yesterday's predictions
    if has_next_day_headlines(date):
        outcomes = collect_next_day_headlines(date)
        scores = evaluate_predictions(
            predictions=get_stored_predictions(date),
            outcomes=outcomes
        )
        
        # 5. Train GSPO on scored data
        training_data = prepare_gspo_data(scores)
        updated_model = train_gspo(
            model=current_model,
            data=training_data,
            epochs=1
        )
        
        # 6. Update model for next day
        current_model = updated_model
        save_checkpoint(updated_model, date)
```

### 3.3  Reward Design Rationale

We use two signals at different points in the loop:

1. **Structure signal** (0–1 at generation time): Binary indicators for required sections (Market Impact, Specific Assets, Trade, Timeframe, Risk, World View). This is tracked as an immediate diagnostic and can drive sampling control; it is not used in the current offline training loss.

2. **Outcome reward** (rank‑derived, mapped to [0,1]): Computed the next day from the evaluator’s strict 1–8 ranking within each headline cohort; reward r_k = 1 − (k−1)/(N−1) with N=8. This reward is used directly for the offline GSPO‑style update as a weight on response token log‑likelihood.

### 3.4  Offline group sequence optimisation (GSPO‑inspired)
We follow the core GSPO-style formulation:

```math
L = -\mathbb{E}_{(x,y)\sim D}\bigl[\sum_t \log p_\theta(y_t \mid x, y_{<t}) \cdot R\bigr]
```

Compared to the GSPO algorithm of Zheng et al. (sequence-likelihood ratios with sequence-level clipping) \cite{Zheng2025GSPO}, our setup differs in three ways: (i) offline, delayed‑reward updates on precomputed roll-outs (no on‑policy sampling); (ii) reward‑weighted token log‑likelihood over response tokens (no sequence‑level importance ratio/clipping); (iii) group‑normalised rewards within headline cohorts to preserve diversity. The high‑level principle—group‑based sequence optimisation—is shared; the optimisation surrogate is adapted to an offline regime on consumer hardware.

#### 3.4.1  Pseudocode

```python
# D : list of (prompt, response, reward)
for prompt, response, R in D:
    tokens = tokenize(prompt + response)
    logits = model(tokens[:-1])
    logp = log_softmax(logits)
    loss = -(logp.gather(index=tokens[1:]).sum()) * R
    loss.backward(); optimizer.step(); optimizer.zero_grad()
```

#### 3.4.2  Implementation details
- Reward shaping: dense, group‑level reward from 8‑way ranking via r_k = 1 − (k−1)/(N−1), N=8.
- Loss masking (default on): policy gradient surrogate is computed only over response tokens (prompt tokens are masked) to align credit assignment with generated text. Toggle‑able for ablation.
- Update schedule: single‑example streaming updates (memory‑efficient) rather than minibatched training.
- Optional variance reduction (default off): EMA reward baseline (momentum 0.9); disabled by default.
- Optimiser: Adam, lr = 1e‑7.
- Sequence length: max_new_tokens = 512; sequences are trimmed to respect model context window.
- Seeding & determinism: seeds set in entry script; MLX determinism is best‑effort on consumer hardware.
- Checkpointing: save every 50 steps; final and dated checkpoints retained.
- Monitoring: per‑step reward and loss proxy logged; CSV exports written for evaluations.

### 3.5  Automatic evaluator architecture

```mermaid
flowchart LR
  P[Roll-out text] --> N[Normalise formatting]
  N --> G[Find-best letter (A–H) via Qwen/MLX]
  G --> X[Regex extract letter → dense reward]
```

Key engineering tweaks that improved ranking completion rate (Section 2.4):

1. Lower temperature (0.3) for more stable outputs during selection.  
2. Regex extractor handles JSON, code-blocks, numbers, bracketed and parenthetical letters.  
3. `<think>` wrapper: chain-of-thought with single-letter final answer.  
4. Deterministic fallback: if stochastic decoding fails to yield a valid letter from the available set, a greedy follow-up prompt requests exactly one letter; this substantially reduces failure cases.

## 4  Related Work

Prior finance-NLP focuses on sentiment/QA (FinBERT \cite{Araci2019FinBERT}, BloombergGPT \cite{Wu2023BloombergGPT}) or price prediction from raw text. RLHF/RLAIF studies (InstructGPT; DPO; KTO) seldom handle **delayed** domain rewards. We bridge the gap by:

- framing news-driven trading as a daily offline RL problem with delayed rewards,
- demonstrating GSPO’s efficacy on <1k examples, and
- scaling reward collection via a cheap automatic evaluator.

### 4.1  Literature Delta Table

| Capability / Work | FinBERT | BloombergGPT | AlphaTrader | ORPO/DPO | **This paper** |
|-------------------|:-------:|:------------:|:-----------:|:--------:|:--------------:|
| Daily feedback (≤ 24 h) | ✗ | ✗ | ✗ | ✗ | **✓** |
| Offline RL on language | ✗ | ✗ | ✗ | ✓ | **✓** |
| Automatic evaluator | ✗ | ✗ | ✗ | ✗ | **✓** |
| Open-source pipeline | ✓ | ✗ | ✗ | ✓ | **✓** |
| Runs on consumer HW | ✓ | ✗ | ✗ | ✓ | **✓** |

This grid highlights that our study combines all traits simultaneously; prior work covers at most two.

### 4.2  Continual & Offline RL for LLMs
- RLHF and instruction-following with human feedback (InstructGPT \cite{Ouyang2022RLHF}; survey \cite{Kaufmann2024RLHFSurvey}).
- Direct optimisation methods (DPO \cite{Rafailov2023DPO}); prospect-theoretic KTO \cite{Ethayarajh2024KTO}.
- Offline RL for LLM reasoning (OREO \cite{Hao2024OREO}), dataset-reset RLHF (DR-PO \cite{Chang2024DRPO}), self-play/SPIN \cite{Chen2024SPIN}.

Our GSPO differs by (i) using delayed, domain-grounded outcome rewards from next-day news, (ii) group‑normalising rewards within headline cohorts to preserve diversity, and (iii) targeting a lightweight, laptop-scale loop.

Comparison to DR-PO and Online Iterative RLHF. DR-PO \cite{Chang2024DRPO} integrates offline preference datasets with online optimisation through dataset resets; it operates on preference judgments and does not rely on exogenous next‑day outcomes. Online Iterative RLHF with a general preference model \cite{Ye2024IterativeRLHF} formalises an on‑policy iterative loop over preferences with KL regularisation. By contrast, our loop uses an exogenous, domain‑grounded delayed reward (next‑day headlines), runs daily on consumer hardware, and applies a simple group‑normalised reward‑weighted MLE that is well‑suited to the offline, delayed regime.

## 5  Experimental Setup

1. **Base model**  Qwen/Qwen3-0.6B (≈2.5 B params after MLX optimisation).  
2. **Hardware**  Apple M4 Max (64-core GPU, 128 GB).  
3. **Dataset**  Five consecutive trading days (Section 1); sixth day reserved as hold-out.  
4. **Baselines**  zero-shot Qwen, supervised MLE, KTO offline RL (planned).  
5. **Metrics**  mean reward (training-time), evaluator pass-through, loss proxy trend (reward-weighted NLL).

### 5.1  Data accounting (post‑fix window)

For the strict‑ranking NEWRUN window (Aug 4–Aug 10), evaluator pass‑through equals generated roll‑outs each day (100%).

| Day       | Headlines | Roll‑outs (8×) | Evaluated | Avg reward |
|----------:|----------:|---------------:|----------:|-----------:|
| 2025‑08‑04 | 32 | 256 | 256 | 0.50 |
| 2025‑08‑05 | 28 | 224 | 224 | 0.50 |
| 2025‑08‑06 | 27 | 216 | 216 | 0.50 |
| 2025‑08‑07 | 35 | 280 | 280 | 0.50 |
| 2025‑08‑08 | 31 | 248 | 248 | 0.50 |
| 2025‑08‑10 | 18 | 144 | 144 | 0.50 |
| **Total**  | 171 | 1,368 | 1,368 | 0.50 |

Note: 2025‑08‑02 contained legacy evaluator noise (including negatives) prior to the strict‑ranking fix; post‑fix days have clean [0,1] rewards.

### 5.2  Training stability diagnostics
We track per‑step reward and a training loss proxy (reward‑weighted negative log‑likelihood over response tokens). Across epochs, average per‑step reward increases monotonically, while the loss proxy rises from ≈4.52 to ≈9.01 (see Fig. `figs/kl_stability.png`), consistent with healthy specialization under a fixed surrogate.

## 6  Preliminary Results (5-day window)

### 6.1  Throughput of the daily loop

Fig. `figs/pipeline_throughput.png` stacks, for each day, the number of roll‑outs that received a usable outcome score (blue) versus those dropped (grey). With strict ranking enabled, pass‑through is 100% for Aug 4–Aug 10.

### 6.2  Learning curve

Fig. `figs/reward_curve.png` plots the per‑step average reward during training. The in‑epoch average increases from ≈0.0135 to ≈0.0292 over 2,592 steps in the post‑fix window.

### 6.3  Statistical considerations

We report descriptive trends from training‑time statistics. Formal hypothesis testing on held‑out behavioral evaluations is left to future work.

### 6.4  Policy stability proxy

The training loss proxy (reward‑weighted NLL) grows moderately (≈4.5→≈9.0), indicating specialization without instability under the fixed surrogate (Fig. `figs/kl_stability.png`).

### 6.5  Baseline comparison (planned)

We will include zero-shot, supervised MLE, and KTO baselines in a future revision alongside a held-out behavioral evaluation set.

## 7  Discussion & Next Steps

Our results demonstrate that lightweight offline RL can continuously improve domain-specific LLM behavior on real-world data. Training-time statistics show steady increases in average reward and a modest rise in the loss proxy during the post-fix window.

**Key findings (post-fix):**
1. **Steady training signal**: Average reward rises across steps; loss proxy increases moderately (≈4.5→≈9.0).
2. **Robust evaluation**: Strict ranking yields clean [0,1] rewards and 100% evaluator pass-through for Aug 4–Aug 10.
3. **Consumer deployment**: Full pipeline runs on a single Apple M4 Max laptop.

### 7.1  Why this is interesting
- Daily, domain‑grounded delayed reward: exogenous next‑day news provides a safe, reproducible signal for offline RL without price feeds or human raters.
- Practical continual tuning on consumer hardware: steady gains with a 0.6B model on a single laptop; stable training dynamics without explicit KL penalties.
- Label‑throughput matters: strict ranking and extractor fixes drive 100% pass‑through post‑fix, directly accelerating learning—an overlooked systems lever.
- Simple surrogate works in a delayed regime: group‑level reward‑weighted MLE provides stable, monotonic improvement without complex RL infrastructure.

**Next steps:**
1. Extend timeline to 30 days; add rolling hold‑out.
2. Complete baselines (SFT/MLE, KTO) and small A/B with sequence‑level surrogate.
3. Add evaluator re‑score ablation and reward‑mix ablation.

## 8  Implications & Generalisation

### 8.1  Transferable lessons

1. Dense, low-cost feedback can outperform sparse, high-quality labels.  
2. Offline RL with pre-computed roll-outs is a safe alternative to on-policy RLHF.  
3. Small models specialise efficiently with task-specific RL.  
4. Label-throughput is a critical optimisation target.  
5. Lightweight stacks (MLX) democratise reinforcement learning for LLMs.

### 8.2  Domain-specific insights

1. Daily‑resolution rewards balance noise vs. sparsity.  
2. Two‑stage evaluator quantifies alignment without price feeds.  
3. Structure + outcome reward synergy improves form & substance.  
4. Training loss proxy trends steadily without explicit KL penalty in this narrow domain.

## 9  Conclusion

We have demonstrated a fully automated daily closed‑loop for financial LLM training using offline reinforcement learning. Our system processes live financial headlines, generates structured predictions, evaluates them against next‑day developments, and continuously improves the model through GSPO‑style updates.

The key innovation is leveraging the natural 24‑hour delay between news events and market outcomes to create a safe offline learning environment. Over the post‑fix window, we observe steady gains in training‑time reward and a modest rise in a loss proxy—validating that lightweight offline RL can continuously improve domain‑specific LLM performance on consumer hardware.

Our work opens new possibilities for democratizing RL techniques while addressing real‑world challenges in financial prediction. The open‑source pipeline provides a reproducible blueprint for domain‑driven offline RL that can be adapted to other time‑sensitive prediction tasks.

## 10  Key Novel Contributions

1. A fully automated **continuous daily closed‑loop** for live financial headlines—roll‑out generation, next‑day market evaluation, and overnight GSPO updating on real noisy data.  
2. Application of **offline Group Sequence Policy Optimisation (GSPO)** on streaming group roll‑outs, normalizing rewards within headline groups to maintain response diversity.  
3. A robust two‑stage LLM evaluator that tolerates format drift and doubles usable label throughput in noisy real‑world settings.  
4. Proof‑of‑concept on a single Apple M4 Max consumer laptop, demonstrating democratized continual RL without cluster resources.

## 11  Limitations & Ethics

**Market impact.**  Model-generated trade ideas could influence prices if widely disseminated, raising manipulation concerns.  We will throttle any public API, log requests, and display disclaimers discouraging real-money use.

**Label noise.**  Headline overlap is an imperfect proxy for genuine trading success.  We will sample 1 % of roll-outs for human grading and plan a price-based evaluator as future work.

**Survivorship bias.**  Some RSS stories are edited or removed post-publication.  Raw XML is snapshot daily and hashed to guarantee auditability.

**Evaluator gaming.**  A learning agent could overfit evaluator quirks.  Counter-measure: rotate evaluator prompts/models monthly and monitor score distribution shifts.

**Financial risk & disclaimers.**  Back-tests include 10 bps transaction cost and serve illustrative purposes only.  The system is released for research, not investment advice.

## A  Reproducibility Checklist

* Code release ✓ (Git repo)  
* Data release RSS archives + scored roll-outs for first 5 days.  
* Hardware Apple M4 Max (documented).  
* Seeds Fixed via `MX_RANDOM_SEED`.  
* Training schedule Single epoch per day over evaluated predictions (see paper/data/training_state.json snapshot).  
* Figure script `python scripts/make_figs.py` regenerates figures and snapshots source data into paper/data/.

To fully reproduce the 5-day experiment:

```bash
# 1. Set up environment
conda env create -f environment.yml   # or pip install -r requirements.txt
conda activate rlfin-llm

# 2. Run the daily pipeline for historical dates
python run_daily_pipeline.py --date 20250802
python run_daily_pipeline.py --date 20250803
# … repeat through 20250806

# 3. Evaluate outcomes (next-day headlines already collected)
python run_evaluation.py --date 20250802
# … repeat per day

# 4. Aggregate and train GSPO
python run_gspo_training.py --training_data training/gspo_training_20250806.json \
    --epochs 1 --save_every 50

# 5. Generate figures
python paper/scripts/make_figs.py
```
