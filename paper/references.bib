@article{Zheng2025GSPO,
  title={Group Sequence Policy Optimization},
  author={Zheng, Chujie and Liu, Shixuan and Li, Mingze and Chen, Xiong-Hui and Yu, Bowen and Gao, Chang and Dang, Kai and Liu, Yuqiong and Men, Rui and Yang, An and Zhou, Jingren and Lin, Junyang},
  journal={arXiv:2507.18071},
  year={2025},
  note={Qwen Team}
}

@inproceedings{Rafailov2023DPO,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafailov, Rafael and Kumar, K and Mitchell, E and others},
  booktitle={NeurIPS},
  year={2023}
}

@article{Ethayarajh2024KTO,
  title={KTO: Model Alignment as Prospect Theoretic Optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv:2402.01306},
  year={2024}
}

@inproceedings{Ouyang2022RLHF,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and others},
  booktitle={NeurIPS},
  year={2022}
}

@article{Zhang2024SelfTuning,
  title={Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching},
  author={Zhang, Xiaoying and Peng, Baolin and Tian, Ye and Zhou, Jingyan and Zhang, Yipeng and Mi, Haitao and Meng, Helen},
  journal={arXiv:2406.06326},
  year={2025},
  note={ACL 2025 Findings}
}

@article{Hao2024OREO,
  title={Offline Reinforcement Learning for LLM Multi-Step Reasoning},
  author={Wang, Huaijie and Hao, Shibo and Dong, Hanze and Zhang, Shenao and Bao, Yilin and Yang, Ziran and Wu, Yi},
  journal={arXiv:2412.16145},
  year={2024}
}

@article{Chang2024DRPO,
  title={Dataset Reset Policy Optimization for RLHF},
  author={Chang, Jonathan D. and Zhan, Wenhao and Oertell, Owen and Brantley, Kiant{\'e} and Misra, Dipendra and Lee, Jason D. and Sun, Wen},
  journal={arXiv:2404.08495},
  year={2024}
}

@article{Chen2024SPIN,
  title={Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models},
  author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
  journal={arXiv:2401.01335},
  year={2024}
}

@article{Kaufmann2024RLHFSurvey,
  title={A Survey of Reinforcement Learning from Human Feedback},
  author={Kaufmann, Timo and Weng, Paul and Bengs, Viktor and H{"u}llermeier, Eyke},
  journal={arXiv:2312.14925},
  year={2024}
}

@article{Baichuan4Finance2024,
  title={Baichuan4-Finance Technical Report},
  author={Zhang, Hanyu and Qiu, Boyu and Feng, Yuhao and Li, Shuqi and Ma, Qian and Zhang, Xiyuan and Ju, Qiang and Yan, Dong and Xie, Jian},
  journal={arXiv:2412.15270},
  year={2025}
}

@inproceedings{Chen2021DecisionTransformer,
  title={Decision Transformer: Reinforcement Learning via Sequence Modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and others},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{Wang2023AlphaTrader,
  title={AlphaTrader: End-to-End Reinforcement Learning for Automated Trading},
  author={Wang, H. and Li, S. and Sun, Z.},
  booktitle={AAAI},
  year={2023}
}

@article{Ye2024IterativeRLHF,
  title={Online Iterative Reinforcement Learning from Human Feedback with General Preference Model},
  author={Ye, Chenlu and Xiong, Wei and Zhang, Yuheng and Jiang, Nan and Zhang, Tong},
  journal={arXiv:2402.07314},
  year={2024}
}
